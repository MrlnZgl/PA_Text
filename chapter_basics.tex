
\chapter{Basics} \label{chap: basics}

This chapter introduces the foundational methods used in this work. Since we process data from \acrfull{md} simulations as input, we present the concept of this modelling strategy. Following, the basics of \acrshort{fe} is presented, because the developed approach is based on a \acrshort{fe} software. The last section outlines the procedure of the Nelder-Mead algorithm which performs the numerical optimisation, and defines the required return value.

\section{Molecular dynamics} \label{sec: MDBasics}
The \acrshort{md} simulation is a frequently used method for biological, chemical and physical investigations on a microscopic level \cite{tuckerman_understanding_2000}. Simulations at the atomistic level allow studies of the structure, interaction processes and energy state of a molecular system \cite{van_gunsteren_computer_1990}.  
In \acrshort{md} simulations, Newton's equation is solved for each atom from the interactions with its neighbouring atoms. These interactions are modelled by potentials. Non-bonded interactions, such as van der Waals potentials, are considered within a cutoff radius \cite{ries_mechanical_2024}. The total potential energy of the system is used to identify the acting forces and accelerations of each particle. To follow the movements of the particles, time integration is necessary. Typical time step sizes are in the femtoseconds range, which makes only small time scales possible with reasonable computational costs \cite{ries_mechanical_2024}. Similar restriction holds for the system size, because of the increasing number of interactions with increasing domain dimensions. However, small dimensions lead to large surface-to-volume ratios which result in significant free surface effects. To avoid them, \acrfull{pbc} are used. They constrain the simulated volume as if it were integrated in an infinitely large domain. Regarding  particle tracking, a particle that leaves the system at one surface, enters the system at the opposite surface. For the deformation of the whole volume element, the \acrshort{pbc} restrict it in such a way that parallel surfaces remain parallel during the loading procedure. These boundary conditions result in a simulation of an infinitely long concatenation of the same volume element in each direction \cite{gorbunov_periodic_2022}.
With these adaptations the results from \acrshort{md} simulations can be transferred to a larger system. 
Thus, \acrshort{md} simulations allow building samples with prescribed properties, followed by deformation tests to study the material behaviour \cite{buyukozturk_structural_2011}. 

\paragraph{Constitutive models}
During a deformation test, stress and strain components are measured for every time step in the simulation. This leads to a series of discrete points that describe the evolution the stresses and strains during the loading process. To deduce general relationships between the stresses and the strains from discrete measurements, constitutive models are required for a mathematical description \cite{mergheim_lecture_nodate}. The material specific properties are considered through material parameters.
Depending on the deformation regime for which the constitutive model holds, different material parameters are useful. The material behaviour of polymers can be represented in an elastoplastic model \cite{ward_mechanical_2013}. Elastoplastic models combine two characteristic material behaviours – elasticity and plastification. An elastic process is characterised by the fact that the loading process is reversible. This means, the loading and unloading processes follow the same path in a stress-strain diagram \cite{mergheim_lecture_nodate}. Generally, the path can follow any function. In combination with plastification, usually linear elastic behaviour is assumed \cite{saabye_ottosen_mechanics_2005}, which can be specified with the material parameters Young's modulus $E$ and Poisson's ratio $\nu$. An elastoplastic material behaves linear-elastic until a stress limit is reached. After that so-called yield stress $\sigma_0$, the material response is irreversible, which leads to the exemplary stress-strain function in \autoref{fig:elastoplasticityCurve} \cite{mergheim_lecture_nodate}. The loading path in the plastic regime can be described by various functions. In the scope of this work, we focus on hardening functions, where the stresses increase with increasing plastic strain \cite{saabye_ottosen_mechanics_2005}. They describe the material behaviour in the plastic regime through multiple plastic material parameters. An important property of elastoplastic material models is their rate-independence. These characteristics lead to identical material behaviour under loading processes with varying strain rates. However, this assumption is not valid for materials with viscous properties. The material response of polymers can include viscous parts as well, meaning their behaviour can be rate-dependent. Therefore, if an elastoplastic constitutive model is to be used, an adequate procedure must be employed to filter out the viscous component of the material response. 


\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{elastoplasticityCurve.pdf}
    \caption{Exemplary stress-strain curve of an elastoplastic model with linear elastic behaviour up to the yield stress $\sigma_0$ and following plastic hardening based on \cite{mergheim_lecture_nodate}}
    \label{fig:elastoplasticityCurve}
\end{figure}


% Elastoplastic models are used for materials, which show elastic characteristics until a yield strength is reached. 

% However, this is only possible, if the viscous parts are neglected. (NUR WEIL VISCO WEGGELASSEN WIRD) In the elastic regime the material  parameters  Young's modulus $E$ and Poisson's ratio $\nu$ are used to specify the material. They show elastic behaviour until a yield strength is reached. 
% Then, the plastification begins which can be described by various hardening models depending on the material characteristics \cite{mergheim_lecture_nodate}. 

\paragraph{Previous work}
This work focusses on the investigations by \citet{ries_deciphering_nodate}, who studied the curing and deformation properties of epoxy through \acrshort{md} simulation. They developed \acrshort{md} models for materials with numerous mixing ratios of resin and hardener\footnote{In the following the mixing ratio is specified in the notation resin:hardener}. The deformation tests from \citet{ries_deciphering_nodate} build the motivation for the here developed optimisation approach. \citet{ries_deciphering_nodate} performed uniaxial tensile tests, loading samples with linear strain up to a maximum value of 20\%. The test samples were constrained by \acrshort{pbc}, which allow lateral contractions. To record the stress and strain components without viscous effects, they developed a procedure to approximate the quasi-static material response. Therefore, only elastic and plastic reactions were considered. Their choice of constitutive models was based on the assumption of isotropic material behaviour. To describe the elastic material behaviour, they used the Neo-Hookean hyperelasticity model. The plastic reactions were modelled by the VOCE-model which defines the stresses during the hardening process through \cite{voce_practical_1948}:


\begin{gather} 
    \sigma = \sigma_0 + \alpha(1 - \text{exp}(-\beta \varepsilon_{pl})) + \gamma \varepsilon_{pl} \label{eq: voce} \\ 
    \sigma_0: \text{ Yield stress} \nonumber\\
    \varepsilon_{pl}: \text{ Plastic strain} \nonumber \\
    \alpha, \beta,  \gamma: \text{ Hardening parameters} \nonumber
\end{gather}


Together with the elastic material parameters Young's modulus $E$ and Poisson's ratio $\nu$, six constitutive parameters were available to fit the stress–strain data measured through \acrshort{md} simulation. The values of the parameters were calculated, using an external minimisation algorithm, which detailed procedure is described in \cite{ries_deciphering_nodate}. The procedure of \citet{ries_deciphering_nodate} is important since their data are used for the model assessment of the optimisation algorithm developed in this work. A detailed description of the optimisation setup is given in \autoref{chap:modelsAndMethods}. In the validation studies of this work, optimisation tests were performed with materials mixing ratios 4:3, 6:3 and 8:3. \citet{ries_deciphering_nodate} also used these mixing ratios to validate their results. Therefore, the optimisation process can easily be evaluated through a comparison of the obtained material parameters. However, a valid comparison requires two conditions: first, that stress and strain data are collected under similar loading conditions, – and – second, that the same constitutive model is used to determine the material parameters. Therefore, a detailed understanding of the methods used by \citet{ries_deciphering_nodate} is essential, since they are adopted for the simulation process used in this work.  



\section{Finite element method} \label{sec: FEMBasics}

The \acrfull{fe} method is a widely used approach in engineering domains like heat transfer in solids, deformations of solids through prescribed loading and interactions between solid structures and fluids \cite{jung_methode_2013}. 
The purpose of \acrshort{fe} method is to find solutions for field problems in complex regimes \cite{willner_vorlesungsskript_nodate}. However, an analytical solution is only possible for simple problem formulations. Therefore, in \acrshort{fe} analysis, the regime is discretised into a finite number of elements. The element behaviour is characterised by approximation functions with a finite number of parameters. Assembling the approximation functions of all elements, leads to an equation system that approximates the solution for the whole regime \cite{jagota_finite_nodate}. The \acrshort{fe} method is primarily used in structural mechanics to provide information about forces and deformations. The general procedure of the \acrshort{fe} method, based on \citet{willner_vorlesungsskript_nodate} and \citet{steinke_finite-elemente-methode_2015}, is presented in the following: 
\begin{align*}
    &\text{1. Discretisation} \\
    &\text{2. Construction of stiffness matrix}\\ 
    &\text{3. Coordinate transformation} \\
    &\text{4. Assembly} \\
    &\text{5. Application of boundary conditions} \\
    &\text{6. Solving equation system}
\end{align*}

In the first step, we discretise the continuum in finite elements. The shape and size of the elements depend on the geometry of the regime, and the required level of precision. To achieve more accurate solutions, smaller elements are necessary. Next, a local stiffness matrix $\boldsymbol{K}$ is created for every element, which connects the acting forces $\boldsymbol{S}$ with the element deformations $\boldsymbol{u}$ via 
\begin{equation}
    \boldsymbol{S} =  \boldsymbol{K} \cdot \boldsymbol{u}.
\end{equation}
Although the equation holds for an element, the calculated forces and deformations are defined at the element nodes. The entries in the stiffness matrix depend on the used element type. They contain material-specific information defined by material parameters. The stiffness matrices are constructed in local coordinate systems. To connect them, a transformation into a global system is necessary. In the fourth step, the equation systems of all elements are combined into a global matrix system. In the assembly, neighbouring elements share their nodes, which needs to be considered during the construction of the equation system. Through prescribed loadings at the boundary of the continuum, certain forces or deformations are known. They are inserted in the equation system as boundary conditions. In the final step the equation system is solved, yielding the displacements for every node \cite{willner_vorlesungsskript_nodate}, \cite{jagota_finite_nodate}. 

\paragraph{Application}
A main advantage of \acrshort{fe} method is its high flexibility. Many different geometries can be modelled through an appropriate choice of element shapes. The accuracy can be adjusted with the element size. Decreasing element sizes lead to more accurate results but require higher computational effort. However, \acrshort{fe} simulations are normally quite fast for easy element geometries. In addition, multiple commercial tools are available for \acrshort{fe} simulations. They offer multiple options to define the properties throughout the entire analysis, which makes them applicable in a wide range of problems. \\
As described in \autoref{chap:introduction}, the aim of this work is to create an approach to determine the material parameters of materials, investigated with \acrshort{md} simulations. Since a \acrshort{fe} simulation requires significantly less computational effort whilst still producing sufficiently accurate results, we decided to base this work on \acrshort{fe} analysis. Therefore, the model used in the \acrshort{md} simulations must be transferred into a \acrshort{fe} model. As reference, we use the \acrshort{md} simulations performed by \citet{ries_deciphering_nodate}. To achieve this, we must transfer the model properties and testing conditions into a \acrshort{fe} environment. In particular, the material behaviour, the boundary conditions, and the applied load must be transferred as exactly as possible. A description of the implementation is given in \autoref{chap:modelsAndMethods}.


\section{Abaqus scripting interface} \label{sec: AbaqusBasics}

The task addressed in this thesis is implemented using \name{Abaqus}/\acrshort{cae} 2024. The commercial software is a widespread tool for \acrshort{fe} analysis. At the Institute of Applied Mechanics (LTM) of the \acrfull{fau}, the software is frequently used, which simplifies subsequent works with the algorithm. In addition, \name{Abaqus} has an integrated \name{Python}-based scripting tool called "Abaqus Scripting Interface". It works as an \acrfull{api} to use the object-oriented programming language \name{Python} in the \name{Abaqus} environment \cite{dassault_systems_abaqus_2015-1}. The "Abaqus Scripting Interface" allows access to the functionalities of \name{Abaqus}/\acrshort{cae} from scripts. Functions, such as the creation and modification of models and jobs, can be controlled via code. Additionally, the output data written for successful executed jobs can be processed \cite{dassault_systems_abaqus_2015-1}. Since it is a \name{Python} extension, the standard programming functionalities are also available. The integration of \name{Abaqus} functionalities in a standard program structure, makes the "Abaqus Scripting Interface" an appropriate tool for the realisation of the task. Because of this property, the implementation is possible in a single script whose structure is shown in \autoref{chap:modelsAndMethods}. The implementation permits a parameter-based analysis which makes value adaptations very fast and easy. If these parameter values are stored in an input file, the user only needs to modify the input-file. This enables fast parameter studies with multiple values, which is used in this work to test multiple combinations of material parameters.  


\subsection{EasyPBC plugin} \label{subsec: EasPBC}

EasyPBC is an \name{Abaqus} plugin which automatically creates \acrshort{pbc}. The plugin was developed by \citet{omairey_development_2019}. It is not an official \name{Abaqus} extension, thus it is not available online. Because of its utilisation in previous works, the plugin was already in use at the institute. The \acrshort{pbc} must constrain parallel surfaces to remain parallel during deformation. To realise this property in \name{Abaqus}, EasyPBC generates node sets of the surface nodes. Opposing nodes are linked via constraint equations to couple their motion. In addition, reference points are created with each one linked to a surface. Therefore, applying load to a reference point causes corresponding reactions on the connected surface which is used for a simplified load application. 

\section{Mathematical basics} \label{sec: mathematics}

To find the material parameters, that best fit the material behaviour measured in the MD-simulation, a mathematical formulation is necessary. This leads to an optimisation problem, where a calculated error, defined as an objective function of the material parameter values, should be minimised. First, we discuss the numerical method to minimise the error, followed by the construction of the error value.

\subsection{Numerical optimisation} \label{subsec: numericaloptimisation}
To solve an optimisation problem various mathematical algorithms are available \cite{rios_derivative-free_2013}. We decided to use the Nelder-Mead algorithm, which is a widely used gradient-free optimisation algorithm \cite{gao_implementing_2012}. In a gradient-free algorithm, the derivatives of the function are neglected in the process. The objective function is based on the results of the \acrshort{fe} analysis, which makes it impossible to determine its derivatives directly. Therefore, only gradient-free algorithms are applicable. In addition, ignoring the derivatives saves significant computational costs, which leads to fast convergence times \cite{pham_comparative_2011}. Because of its simple structure, the algorithm is a standard feature in many numerical libraries \cite{singer_efficient_2004}. In \name{Python}, it is available in the \verb|scipy.minimize()| function. The function call is described in detail in \autoref{sec: optimisationCode}. Here, we focus on the procedure of the numerical algorithm, which is visualised in \autoref{fig:nelderMead}. The Nelder-Mead algorithm is capable of finding a local minimum of a scalar function, depending on $n$ optimisation variables. In this work, the optimisation variables are the material parameters. The definition of the objective function can be found in \autoref{chap:modelsAndMethods}. Assuming the objective function is known, the first step is to create $n+1$ points $\mathbf{P}$ in an $n$-dimensional space. In \autoref{fig:nelderMead}, an exemplary numerical optimisation with three optimisation variables is depicted. First, the positions of the points must be determined. This is done by an initial guess $\hat{x}$ for every optimisation variable. To process three optimisation variables, the initial guess would look like this: 

\begin{gather}\label{eq:initialPoinsNelderMead}
    \mathbf{\hat{x}} = [\hat{x}^0, \hat{x}^1, \hat{x}^2] \\
    \text{with } \hat{x}^i \equiv \text{initial guess of the $i$-th optimisation variable} \nonumber
\end{gather}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.85\textwidth]{NelderMeadAlg.pdf}
    \caption{Pocedure of the Nelder-Mead algorithm for an exemplary optimisation with three optimisation variables}
    \label{fig:nelderMead}
\end{figure}

Based on \autoref{eq:initialPoinsNelderMead}, the initial points $\mathbf{\hat{P}_i}$ are constructed. The first one is defined as $\mathbf{\hat{P}_1} = \mathbf{\hat{x}}$. For the other points, the value of one variable in the initial guess is changed each. The points are connected to each other in such a way that an $n$-dimensional simplex is created. A simplex is a general geometric object in $n$-dimensional space consisting of $n+1$ points. At the top of \autoref{fig:nelderMead} it can be seen, that in two-dimensional space a simplex is equivalent to a triangle.
In the next step, the function values corresponding to the points $\mathbf{P_i}$ are evaluated and sorted by size. The highest function value $y_h$ maps the worst value combination $\mathbf{P_h}$ of the optimisation parameters, as signed at the upper triangle in \autoref{fig:nelderMead}. When the algorithm starts, a centroid of all points of the simplex except $\mathbf{P_h}$ is determined. At this point $\mathbf{P_C}$, $\mathbf{P_h}$ is reflected at a new position $\mathbf{P_R}$, which is depicted in the second triangle in \autoref{fig:nelderMead}. Before the new point $\mathbf{P^{*}}$ is positioned, the corresponding function value $y(\mathbf{P_R})$ needs to be evaluated. Depending on its value, one of the four possible operations presented in \autoref{fig:nelderMead} is executed. 
The operation \benennung{Reflection} is performed, if $y(\mathbf{P_R})$ is smaller than the second-worst function value. If $y(\mathbf{P_R})$ is even smaller than $y(\mathbf{P_0})$, which is currently the best value, an \benennung{Expansion} is performed. In the other cases, one of the \benennung{Contraction} operations is executed. The worst case occurs, if both \benennung{Contraction} operations bring an increased function value $y(\mathbf{P^*})$ compared to $y(\mathbf{P_R})$ and $y(\mathbf{P_h})$. As a consequence, a \benennung{Shrinking} operation is required to minimise the whole simplex towards the currently best position. This is shown in the bottom right of \autoref{fig:nelderMead}, where $\mathbf{P_1}$ and $\mathbf{P_2}$ are moved closer to the current minimum point $\mathbf{P_0}$. To choose the correct operation depending on the current simplex, multiple evaluations of $y$ at different points $\mathbf{P}$ might be necessary. Thus, multiple function evaluations are accomplished during one optimisation iteration. When an improved position $\mathbf{P^{*}}$ is found, the algorithm starts again with the new simplex \cite{nelder_simplex_1965}. If the variations of the functions values $y_i$ meet a certain lower limit, the minimum of the function with its corresponding parameter is found which is marked at the bottom of \autoref{fig:nelderMead}.

To ensure a successful search, the initial simplex should be scaled regularly \cite{baudin_nelder-mead_nodate} which is possible through a regular distribution of the points $\mathbf{\hat{P}_i}$ in space. This can be difficult if the values of the optimisation variables differ greatly in size. Therefore, it is necessary to normalise the variable values within the range of 0 to 1. The algorithm is vulnerable to becoming stuck in local minima because of the \benennung{Shrinking} operation \cite{luersen_globalized_2004}. Therefore, a smart choice of initial values is helpful, to avoid starting points near a local minimum. However, if the trend of the objective function is unknown, this can be challenging. In addition, the number of optimisation parameters should be constrained. So far, stable convergence behaviour of the Nelder-Mead algorithm has mostly been studied for small numbers of variables \cite{singer_efficient_2004}, \cite{pham_comparative_2011}.



\subsection{Root mean squared error} \label{subsec: RMSE}
To use the Nelder-Mead algorithm, we need to construct an objective function that returns a scalar value. It should be preempted at this point that several values must be minimised for an adequate optimisation result. In order to handle this issue, it is necessary to condense all applicable data into a single value. As a representative value, we choose the \acrfull{rmse}. It is a frequently used value to express variations of two data sets - usually a reference data set $f$ and a test data set $\hat{f}$ \cite{morrow_method_2010}. The \acrshort{rmse} is based on the difference between the data points at position $i$. This deviation is composed for all points, and then combined in the following formula

\begin{equation} \label{eq: RMSE}
    \text{RMSE} = \sqrt{\frac{1}{N}\sum_{i=1}^{N} (f_i - \hat{f}_i)^2}
\end{equation}

With this definition, the proportion of all deviations in the \acrshort{rmse} is equal. The value of the \acrshort{rmse} is always positive and tends towards zero for perfectly matching data sets. In addition, the unit of the \acrshort{rmse} matches that of the data points. These characteristics are advantageous for the evaluation of its value. Since we must include the deviations between $M$ multiple data sets, we extend the formulation to 

\begin{equation} \label{eq: multiRMSE}
    \text{RMSE} = \sqrt{\frac{1}{M} \sum_{j=1}^{M} \left[ \frac{1}{N} \sum_{i=1}^{N} (f_i - \hat{f}_i)^2 \right] _j}
\end{equation}

In this formulation, the \acrshort{rmse} is only unit-based, if all data sets have the same unit. Otherwise, the units of each data set are neutralised through weights with the corresponding inverse unit. Therefore, the resulting \acrshort{rmse} is unitless. The application of \autoref{eq: multiRMSE} in the developed algorithm is explained in \autoref{sec: errorCalculation}. 


